INFO 04-14 08:38:50 __init__.py:190] Automatically detected platform cuda.
INFO 04-14 08:38:52 api_server.py:840] vLLM API server version 0.7.2
INFO 04-14 08:38:52 api_server.py:841] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='Qwen/Qwen2.5-14B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='half', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)
INFO 04-14 08:38:52 api_server.py:206] Started engine process with PID 1031142
WARNING 04-14 08:38:53 config.py:2386] Casting torch.bfloat16 to torch.float16.
INFO 04-14 08:38:57 __init__.py:190] Automatically detected platform cuda.
WARNING 04-14 08:39:01 config.py:2386] Casting torch.bfloat16 to torch.float16.
INFO 04-14 08:39:01 config.py:542] This model supports multiple tasks: {'generate', 'score', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.
INFO 04-14 08:39:01 config.py:1401] Defaulting to use mp for distributed inference
INFO 04-14 08:39:08 config.py:542] This model supports multiple tasks: {'classify', 'score', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.
INFO 04-14 08:39:08 config.py:1401] Defaulting to use mp for distributed inference
INFO 04-14 08:39:08 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='Qwen/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
WARNING 04-14 08:39:08 multiproc_worker_utils.py:300] Reducing Torch parallelism from 12 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 04-14 08:39:08 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=1031294)[0;0m INFO 04-14 08:39:08 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=1031294)[0;0m INFO 04-14 08:39:09 cuda.py:230] Using Flash Attention backend.
INFO 04-14 08:39:09 cuda.py:230] Using Flash Attention backend.
INFO 04-14 08:39:10 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=1031294)[0;0m INFO 04-14 08:39:10 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=1031294)[0;0m INFO 04-14 08:39:10 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 04-14 08:39:10 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 04-14 08:39:11 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /storage/home/hcoda1/7/avandevoorde3/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorkerProcess pid=1031294)[0;0m INFO 04-14 08:39:11 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /storage/home/hcoda1/7/avandevoorde3/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 04-14 08:39:11 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_4573dd50'), local_subscribe_port=42175, remote_subscribe_port=None)
INFO 04-14 08:39:11 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(VllmWorkerProcess pid=1031294)[0;0m INFO 04-14 08:39:11 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-14B-Instruct...
INFO 04-14 08:39:11 weight_utils.py:252] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=1031294)[0;0m INFO 04-14 08:39:11 weight_utils.py:252] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:01<00:13,  1.97s/it]
Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:03<00:11,  1.96s/it]
Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:05<00:09,  1.94s/it]
Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:06<00:05,  1.47s/it]
Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:08<00:04,  1.63s/it]
Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:10<00:03,  1.74s/it]
Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:12<00:01,  1.78s/it]
Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:14<00:00,  1.86s/it]
Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:14<00:00,  1.79s/it]

INFO 04-14 08:39:26 model_runner.py:1115] Loading model weights took 13.9281 GB
[1;36m(VllmWorkerProcess pid=1031294)[0;0m INFO 04-14 08:39:26 model_runner.py:1115] Loading model weights took 13.9281 GB
[1;36m(VllmWorkerProcess pid=1031294)[0;0m INFO 04-14 08:39:33 worker.py:267] Memory profiling takes 6.63 seconds
[1;36m(VllmWorkerProcess pid=1031294)[0;0m INFO 04-14 08:39:33 worker.py:267] the current vLLM instance can use total_gpu_memory (39.50GiB) x gpu_memory_utilization (0.90) = 35.55GiB
[1;36m(VllmWorkerProcess pid=1031294)[0;0m INFO 04-14 08:39:33 worker.py:267] model weights take 13.93GiB; non_torch_memory takes 1.54GiB; PyTorch activation peak memory takes 2.84GiB; the rest of the memory reserved for KV Cache is 17.24GiB.
INFO 04-14 08:39:33 worker.py:267] Memory profiling takes 6.68 seconds
INFO 04-14 08:39:33 worker.py:267] the current vLLM instance can use total_gpu_memory (39.50GiB) x gpu_memory_utilization (0.90) = 35.55GiB
INFO 04-14 08:39:33 worker.py:267] model weights take 13.93GiB; non_torch_memory takes 1.54GiB; PyTorch activation peak memory takes 2.84GiB; the rest of the memory reserved for KV Cache is 17.24GiB.
INFO 04-14 08:39:33 executor_base.py:110] # CUDA blocks: 11770, # CPU blocks: 2730
INFO 04-14 08:39:33 executor_base.py:115] Maximum concurrency for 32768 tokens per request: 5.75x
INFO 04-14 08:39:35 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=1031294)[0;0m INFO 04-14 08:39:35 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:17,  1.90it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:15,  2.06it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:14,  2.13it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:02<00:13,  2.18it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:02<00:13,  2.18it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:03<00:12,  2.20it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:03<00:12,  2.21it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:04<00:11,  2.23it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:04<00:11,  2.24it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:05<00:10,  2.25it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:05<00:10,  2.26it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:05<00:09,  2.24it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:06<00:09,  2.25it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:06<00:08,  2.26it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:07<00:08,  2.37it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:07<00:07,  2.50it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:07<00:06,  2.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:08<00:05,  2.67it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:08<00:05,  2.74it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:08<00:05,  2.78it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:09<00:04,  2.77it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:09<00:04,  2.80it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:10<00:03,  2.78it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:10<00:03,  2.83it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:10<00:03,  2.86it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:11<00:02,  2.89it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:11<00:02,  2.90it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:11<00:02,  2.91it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:12<00:01,  2.91it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:12<00:01,  2.91it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:12<00:01,  2.92it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:13<00:00,  2.93it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:13<00:00,  2.93it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:14<00:00,  2.16it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:14<00:00,  2.46it/s]
INFO 04-14 08:39:49 custom_all_reduce.py:226] Registering 3395 cuda graph addresses
[1;36m(VllmWorkerProcess pid=1031294)[0;0m INFO 04-14 08:39:49 custom_all_reduce.py:226] Registering 3395 cuda graph addresses
[1;36m(VllmWorkerProcess pid=1031294)[0;0m INFO 04-14 08:39:49 model_runner.py:1562] Graph capturing finished in 14 secs, took 0.28 GiB
INFO 04-14 08:39:49 model_runner.py:1562] Graph capturing finished in 14 secs, took 0.28 GiB
INFO 04-14 08:39:49 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 23.12 seconds
INFO 04-14 08:39:49 api_server.py:756] Using supplied chat template:
INFO 04-14 08:39:49 api_server.py:756] None
INFO 04-14 08:39:49 launcher.py:21] Available routes are:
INFO 04-14 08:39:49 launcher.py:29] Route: /openapi.json, Methods: HEAD, GET
INFO 04-14 08:39:49 launcher.py:29] Route: /docs, Methods: HEAD, GET
INFO 04-14 08:39:49 launcher.py:29] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 04-14 08:39:49 launcher.py:29] Route: /redoc, Methods: HEAD, GET
INFO 04-14 08:39:49 launcher.py:29] Route: /health, Methods: GET
INFO 04-14 08:39:49 launcher.py:29] Route: /ping, Methods: POST, GET
INFO 04-14 08:39:49 launcher.py:29] Route: /tokenize, Methods: POST
INFO 04-14 08:39:49 launcher.py:29] Route: /detokenize, Methods: POST
INFO 04-14 08:39:49 launcher.py:29] Route: /v1/models, Methods: GET
INFO 04-14 08:39:49 launcher.py:29] Route: /version, Methods: GET
INFO 04-14 08:39:49 launcher.py:29] Route: /v1/chat/completions, Methods: POST
INFO 04-14 08:39:49 launcher.py:29] Route: /v1/completions, Methods: POST
INFO 04-14 08:39:49 launcher.py:29] Route: /v1/embeddings, Methods: POST
INFO 04-14 08:39:49 launcher.py:29] Route: /pooling, Methods: POST
INFO 04-14 08:39:49 launcher.py:29] Route: /score, Methods: POST
INFO 04-14 08:39:49 launcher.py:29] Route: /v1/score, Methods: POST
INFO 04-14 08:39:49 launcher.py:29] Route: /rerank, Methods: POST
INFO 04-14 08:39:49 launcher.py:29] Route: /v1/rerank, Methods: POST
INFO 04-14 08:39:49 launcher.py:29] Route: /v2/rerank, Methods: POST
INFO 04-14 08:39:49 launcher.py:29] Route: /invocations, Methods: POST
INFO:     Started server process [1031110]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     ::1:41164 - "GET /v1/models HTTP/1.1" 200 OK
INFO 04-14 08:40:15 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
INFO 04-14 08:40:15 logger.py:39] Received request chatcmpl-83b9ced035e241348041c6b788ff79d1: prompt: "<|im_start|>system\nYou are in a role play game. The following roles are available:\nWriter: Writer for creating any text content.\nEditor: Editor for planning and reviewing the content..\nRead the following conversation. Then select the next role from ['Writer', 'Editor'] to play. Only return the role.\n\nUser: Please write a short story about the gingerbread in halloween!\n\nRead the above conversation. Then select the next role from ['Writer', 'Editor'] to play. if you think it's enough talking (for example they have talked for 3 rounds), return 'FINISH'.\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32644, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 04-14 08:40:15 engine.py:275] Added request chatcmpl-83b9ced035e241348041c6b788ff79d1.
INFO 04-14 08:40:16 metrics.py:455] Avg prompt throughput: 19.3 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO:     ::1:57804 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-14 08:40:16 logger.py:39] Received request chatcmpl-d41843d81cca4921bd087d30ddbf2a3b: prompt: '<|im_start|>system\nYou are a one sentence Writer and provide one sentence content each time<|im_end|>\n<|im_start|>user\nTransferred to User<|im_end|>\n<|im_start|>user\nPlease write a short story about the gingerbread in halloween!<|im_end|>\n<|im_start|>user\nTransferred to Writer, adopt the persona immediately.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32705, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 04-14 08:40:16 engine.py:275] Added request chatcmpl-d41843d81cca4921bd087d30ddbf2a3b.
INFO:     ::1:57818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-14 08:40:16 logger.py:39] Received request chatcmpl-ff660f85f13549459c1621b3a2ea0d53: prompt: '<|im_start|>system\n\n            Please provide updates to the state based on your last message and the previous state, if any.\n            Use the following JSON format, replacing the \'None\' value with the actual value.\n            {\n                "writer_topic": "None",\n            }\n            <|im_end|>\n<|im_start|>assistant\nOn Halloween night, the gingerbread house in the old Smith\'s yard came to life, its candy bricks glowing in the moonlight as it whispered sweet, eerie invitations to the passersby.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32665, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 04-14 08:40:16 engine.py:275] Added request chatcmpl-ff660f85f13549459c1621b3a2ea0d53.
INFO:     ::1:57818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-14 08:40:17 logger.py:39] Received request chatcmpl-61deb313a5974a41b4a4e8c94fe6978b: prompt: '<|im_start|>system\n\n            Please provide updates to the state based on your last message and the previous state, if any.\n            Use the following JSON format, replacing the \'None\' value with the actual value.\n            {\n                "writer_topic": "None",\n            }\n            <|im_end|>\n<|im_start|>assistant\nOn Halloween night, the gingerbread house in the old Smith\'s yard came to life, its candy bricks glowing in the moonlight as it whispered sweet, eerie invitations to the passersby.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32665, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 04-14 08:40:17 engine.py:275] Added request chatcmpl-61deb313a5974a41b4a4e8c94fe6978b.
INFO:     ::1:57818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-14 08:40:19 logger.py:39] Received request chatcmpl-5b133268975b45e0af74967c5c63fb44: prompt: "<|im_start|>system\nYou are in a role play game. The following roles are available:\nEditor: Editor for planning and reviewing the content..\nRead the following conversation. Then select the next role from ['Editor'] to play. Only return the role.\n\nUser: Please write a short story about the gingerbread in halloween!\nWriter: On Halloween night, the gingerbread house in the old Smith's yard came to life, its candy bricks glowing in the moonlight as it whispered sweet, eerie invitations to the passersby.\n\nRead the above conversation. Then select the next role from ['Editor'] to play. if you think it's enough talking (for example they have talked for 3 rounds), return 'FINISH'.\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32618, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 04-14 08:40:19 engine.py:275] Added request chatcmpl-5b133268975b45e0af74967c5c63fb44.
INFO:     ::1:57804 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-14 08:40:20 logger.py:39] Received request chatcmpl-b4a4eb7f922241f1954134414a2a857e: prompt: "<|im_start|>system\nYou are an Editor. You provide just max 15 words as feedback on writers content.<|im_end|>\n<|im_start|>user\nTransferred to User<|im_end|>\n<|im_start|>user\nPlease write a short story about the gingerbread in halloween!<|im_end|>\n<|im_start|>user\nTransferred to Writer<|im_end|>\n<|im_start|>user\nOn Halloween night, the gingerbread house in the old Smith's yard came to life, its candy bricks glowing in the moonlight as it whispered sweet, eerie invitations to the passersby.<|im_end|>\n<|im_start|>user\nTransferred to Editor, adopt the persona immediately.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32646, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 04-14 08:40:20 engine.py:275] Added request chatcmpl-b4a4eb7f922241f1954134414a2a857e.
INFO:     ::1:46916 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-14 08:40:20 logger.py:39] Received request chatcmpl-ab564286373042ae95da8241625239f4: prompt: '<|im_start|>system\n\n            Please provide updates to the state based on your last message and the previous state, if any.\n            Use the following JSON format, replacing the \'None\' value with the actual value.\n            {\n                "writer_topic": "None",\n            }\n            <|im_end|>\n<|im_start|>assistant\nEngaging start; consider deepening gingerbread\'s role and interactions.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32690, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 04-14 08:40:20 engine.py:275] Added request chatcmpl-ab564286373042ae95da8241625239f4.
INFO:     ::1:46916 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-14 08:40:21 logger.py:39] Received request chatcmpl-d0186e5efb6a469082f8fa6732c54882: prompt: "<|im_start|>system\nYou are in a role play game. The following roles are available:\nWriter: Writer for creating any text content..\nRead the following conversation. Then select the next role from ['Writer'] to play. Only return the role.\n\nUser: Please write a short story about the gingerbread in halloween!\nWriter: On Halloween night, the gingerbread house in the old Smith's yard came to life, its candy bricks glowing in the moonlight as it whispered sweet, eerie invitations to the passersby.\nEditor: Engaging start; consider deepening gingerbread's role and interactions.\n\nRead the above conversation. Then select the next role from ['Writer'] to play. if you think it's enough talking (for example they have talked for 3 rounds), return 'FINISH'.\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32603, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 04-14 08:40:21 engine.py:275] Added request chatcmpl-d0186e5efb6a469082f8fa6732c54882.
INFO 04-14 08:40:21 metrics.py:455] Avg prompt throughput: 146.5 tokens/s, Avg generation throughput: 24.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO:     ::1:57804 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-14 08:40:21 logger.py:39] Received request chatcmpl-fd0360ed5c4743f487dd79fb52567015: prompt: "<|im_start|>system\nYou are a one sentence Writer and provide one sentence content each time<|im_end|>\n<|im_start|>user\nTransferred to User<|im_end|>\n<|im_start|>user\nPlease write a short story about the gingerbread in halloween!<|im_end|>\n<|im_start|>user\nTransferred to Writer, adopt the persona immediately.<|im_end|>\n<|im_start|>assistant\nOn Halloween night, the gingerbread house in the old Smith's yard came to life, its candy bricks glowing in the moonlight as it whispered sweet, eerie invitations to the passersby.<|im_end|>\n<|im_start|>user\nTransferred to Editor<|im_end|>\n<|im_start|>user\nEngaging start; consider deepening gingerbread's role and interactions.<|im_end|>\n<|im_start|>user\nTransferred to Writer, adopt the persona immediately.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32618, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 04-14 08:40:21 engine.py:275] Added request chatcmpl-fd0360ed5c4743f487dd79fb52567015.
INFO:     ::1:57818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-14 08:40:22 logger.py:39] Received request chatcmpl-b3c441193e5343c0a412d9577890c44b: prompt: '<|im_start|>system\n\n            Please provide updates to the state based on your last message and the previous state, if any.\n            Use the following JSON format, replacing the \'None\' value with the actual value.\n            {\n                "writer_topic": "None",\n            }\n            <|im_end|>\n<|im_start|>assistant\n{\n    "writer_topic": "Halloween-themed storytelling with a gingerbread house that comes to life at night, targeting an audience interested in magical and eerie tales."\n}<|im_end|>\n<|im_start|>assistant\nOn Halloween night, the gingerbread house in the old Smith\'s yard came to life, its candy bricks glowing in the moonlight, and it began to whisper sweet, eerie invitations to the ghosts and goblins who passed by, offering them a taste of its sugary delights in exchange for haunting tales from beyond the grave.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 04-14 08:40:22 engine.py:275] Added request chatcmpl-b3c441193e5343c0a412d9577890c44b.
INFO:     ::1:57818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-14 08:40:27 logger.py:39] Received request chatcmpl-da3e4278f35949c7b159b323c0d95f36: prompt: "<|im_start|>system\nYou are in a role play game. The following roles are available:\nEditor: Editor for planning and reviewing the content..\nRead the following conversation. Then select the next role from ['Editor'] to play. Only return the role.\n\nUser: Please write a short story about the gingerbread in halloween!\nWriter: On Halloween night, the gingerbread house in the old Smith's yard came to life, its candy bricks glowing in the moonlight as it whispered sweet, eerie invitations to the passersby.\nEditor: Engaging start; consider deepening gingerbread's role and interactions.\nWriter: On Halloween night, the gingerbread house in the old Smith's yard came to life, its candy bricks glowing in the moonlight, and it began to whisper sweet, eerie invitations to the ghosts and goblins who passed by, offering them a taste of its sugary delights in exchange for haunting tales from beyond the grave.\n\nRead the above conversation. Then select the next role from ['Editor'] to play. if you think it's enough talking (for example they have talked for 3 rounds), return 'FINISH'.\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32535, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 04-14 08:40:27 engine.py:275] Added request chatcmpl-da3e4278f35949c7b159b323c0d95f36.
INFO 04-14 08:40:27 metrics.py:455] Avg prompt throughput: 97.3 tokens/s, Avg generation throughput: 19.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO:     ::1:46928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-14 08:40:27 logger.py:39] Received request chatcmpl-3dbe0574b3404123aa6e3da5d3ba514c: prompt: "<|im_start|>system\nYou are an Editor. You provide just max 15 words as feedback on writers content.<|im_end|>\n<|im_start|>user\nTransferred to User<|im_end|>\n<|im_start|>user\nPlease write a short story about the gingerbread in halloween!<|im_end|>\n<|im_start|>user\nTransferred to Writer<|im_end|>\n<|im_start|>user\nOn Halloween night, the gingerbread house in the old Smith's yard came to life, its candy bricks glowing in the moonlight as it whispered sweet, eerie invitations to the passersby.<|im_end|>\n<|im_start|>user\nTransferred to Editor, adopt the persona immediately.<|im_end|>\n<|im_start|>assistant\nEngaging start; consider deepening gingerbread's role and interactions.<|im_end|>\n<|im_start|>user\nTransferred to Writer<|im_end|>\n<|im_start|>user\nOn Halloween night, the gingerbread house in the old Smith's yard came to life, its candy bricks glowing in the moonlight, and it began to whisper sweet, eerie invitations to the ghosts and goblins who passed by, offering them a taste of its sugary delights in exchange for haunting tales from beyond the grave.<|im_end|>\n<|im_start|>user\nTransferred to Editor, adopt the persona immediately.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32533, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 04-14 08:40:27 engine.py:275] Added request chatcmpl-3dbe0574b3404123aa6e3da5d3ba514c.
INFO:     ::1:46940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-14 08:40:27 logger.py:39] Received request chatcmpl-e9b426d0436e4bb48de7454d4d1fd019: prompt: '<|im_start|>system\n\n            Please provide updates to the state based on your last message and the previous state, if any.\n            Use the following JSON format, replacing the \'None\' value with the actual value.\n            {\n                "writer_topic": "None",\n            }\n            <|im_end|>\n<|im_start|>assistant\n{\n    "writer_topic": "Exploring deeper interactions and character development for gingerbread"\n}<|im_end|>\n<|im_start|>assistant\nVivid and eerie; enhance the mystical atmosphere, deepen the gingerbread\'s character.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32663, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 04-14 08:40:27 engine.py:275] Added request chatcmpl-e9b426d0436e4bb48de7454d4d1fd019.
INFO:     ::1:46940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-14 08:40:28 logger.py:39] Received request chatcmpl-f6094c5a86a74c63a19d83cec37c40f2: prompt: "<|im_start|>system\nYou are in a role play game. The following roles are available:\nWriter: Writer for creating any text content..\nRead the following conversation. Then select the next role from ['Writer'] to play. Only return the role.\n\nUser: Please write a short story about the gingerbread in halloween!\nWriter: On Halloween night, the gingerbread house in the old Smith's yard came to life, its candy bricks glowing in the moonlight as it whispered sweet, eerie invitations to the passersby.\nEditor: Engaging start; consider deepening gingerbread's role and interactions.\nWriter: On Halloween night, the gingerbread house in the old Smith's yard came to life, its candy bricks glowing in the moonlight, and it began to whisper sweet, eerie invitations to the ghosts and goblins who passed by, offering them a taste of its sugary delights in exchange for haunting tales from beyond the grave.\nEditor: Vivid and eerie; enhance the mystical atmosphere, deepen the gingerbread's character.\n\nRead the above conversation. Then select the next role from ['Writer'] to play. if you think it's enough talking (for example they have talked for 3 rounds), return 'FINISH'.\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32517, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 04-14 08:40:28 engine.py:275] Added request chatcmpl-f6094c5a86a74c63a19d83cec37c40f2.
INFO:     ::1:46928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-14 08:40:31 logger.py:39] Received request chatcmpl-c224aa4b24fa4430b461abc4147c8fca: prompt: "<|im_start|>system\nYou are a one sentence Writer and provide one sentence content each time<|im_end|>\n<|im_start|>user\nTransferred to User<|im_end|>\n<|im_start|>user\nPlease write a short story about the gingerbread in halloween!<|im_end|>\n<|im_start|>user\nTransferred to Writer, adopt the persona immediately.<|im_end|>\n<|im_start|>assistant\nOn Halloween night, the gingerbread house in the old Smith's yard came to life, its candy bricks glowing in the moonlight as it whispered sweet, eerie invitations to the passersby.<|im_end|>\n<|im_start|>user\nTransferred to Editor<|im_end|>\n<|im_start|>user\nEngaging start; consider deepening gingerbread's role and interactions.<|im_end|>\n<|im_start|>user\nTransferred to Writer, adopt the persona immediately.<|im_end|>\n<|im_start|>assistant\nOn Halloween night, the gingerbread house in the old Smith's yard came to life, its candy bricks glowing in the moonlight, and it began to whisper sweet, eerie invitations to the ghosts and goblins who passed by, offering them a taste of its sugary delights in exchange for haunting tales from beyond the grave.<|im_end|>\n<|im_start|>user\nTransferred to Editor<|im_end|>\n<|im_start|>user\nVivid and eerie; enhance the mystical atmosphere, deepen the gingerbread's character.<|im_end|>\n<|im_start|>user\nTransferred to Writer, adopt the persona immediately.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32502, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 04-14 08:40:31 engine.py:275] Added request chatcmpl-c224aa4b24fa4430b461abc4147c8fca.
INFO 04-14 08:40:32 metrics.py:455] Avg prompt throughput: 171.3 tokens/s, Avg generation throughput: 51.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO:     ::1:56230 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-14 08:40:32 logger.py:39] Received request chatcmpl-751fc2c0e06a44ed9aa2d3ab9fa43f67: prompt: '<|im_start|>system\n\n            Please provide updates to the state based on your last message and the previous state, if any.\n            Use the following JSON format, replacing the \'None\' value with the actual value.\n            {\n                "writer_topic": "None",\n            }\n            <|im_end|>\n<|im_start|>assistant\n{\n    "writer_topic": "Halloween-themed storytelling with a gingerbread house that comes to life at night, targeting an audience interested in magical and eerie tales."\n}<|im_end|>\n<|im_start|>assistant\n{\n    "writer_topic": "Halloween-themed storytelling with a magical and eerie twist, where a gingerbread house in the old Smith\'s yard comes to life and invites ghosts and goblins with haunting tales."\n}<|im_end|>\n<|im_start|>assistant\nOn Halloween night, the gingerbread house in the old Smith\'s yard glowed with an otherworldly light, its candy bricks shimmering in the moonlight. The house, now a mystical entity, whispered sweet, eerie invitations to the ghosts and goblins that passed by, offering them enchanted treats in exchange for haunting tales from beyond the grave, each story weaving deeper into the gingerbread\'s ancient, magical essence.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32533, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 04-14 08:40:32 engine.py:275] Added request chatcmpl-751fc2c0e06a44ed9aa2d3ab9fa43f67.
INFO:     ::1:56230 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-14 08:40:38 logger.py:39] Received request chatcmpl-8957761dfa0a40ecacc8186d0c622488: prompt: "<|im_start|>system\nYou are in a role play game. The following roles are available:\nEditor: Editor for planning and reviewing the content..\nRead the following conversation. Then select the next role from ['Editor'] to play. Only return the role.\n\nUser: Please write a short story about the gingerbread in halloween!\nWriter: On Halloween night, the gingerbread house in the old Smith's yard came to life, its candy bricks glowing in the moonlight as it whispered sweet, eerie invitations to the passersby.\nEditor: Engaging start; consider deepening gingerbread's role and interactions.\nWriter: On Halloween night, the gingerbread house in the old Smith's yard came to life, its candy bricks glowing in the moonlight, and it began to whisper sweet, eerie invitations to the ghosts and goblins who passed by, offering them a taste of its sugary delights in exchange for haunting tales from beyond the grave.\nEditor: Vivid and eerie; enhance the mystical atmosphere, deepen the gingerbread's character.\nWriter: On Halloween night, the gingerbread house in the old Smith's yard glowed with an otherworldly light, its candy bricks shimmering in the moonlight. The house, now a mystical entity, whispered sweet, eerie invitations to the ghosts and goblins that passed by, offering them enchanted treats in exchange for haunting tales from beyond the grave, each story weaving deeper into the gingerbread's ancient, magical essence.\n\nRead the above conversation. Then select the next role from ['Editor'] to play. if you think it's enough talking (for example they have talked for 3 rounds), return 'FINISH'.\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32430, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 04-14 08:40:38 engine.py:275] Added request chatcmpl-8957761dfa0a40ecacc8186d0c622488.
INFO 04-14 08:40:38 metrics.py:455] Avg prompt throughput: 90.9 tokens/s, Avg generation throughput: 12.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO:     ::1:41698 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-14 08:40:38 logger.py:39] Received request chatcmpl-0b9ef05b8bb746fb860558966ad7bd89: prompt: "<|im_start|>system\nYou are an Editor. You provide just max 15 words as feedback on writers content.<|im_end|>\n<|im_start|>user\nTransferred to User<|im_end|>\n<|im_start|>user\nPlease write a short story about the gingerbread in halloween!<|im_end|>\n<|im_start|>user\nTransferred to Writer<|im_end|>\n<|im_start|>user\nOn Halloween night, the gingerbread house in the old Smith's yard came to life, its candy bricks glowing in the moonlight as it whispered sweet, eerie invitations to the passersby.<|im_end|>\n<|im_start|>user\nTransferred to Editor, adopt the persona immediately.<|im_end|>\n<|im_start|>assistant\nEngaging start; consider deepening gingerbread's role and interactions.<|im_end|>\n<|im_start|>user\nTransferred to Writer<|im_end|>\n<|im_start|>user\nOn Halloween night, the gingerbread house in the old Smith's yard came to life, its candy bricks glowing in the moonlight, and it began to whisper sweet, eerie invitations to the ghosts and goblins who passed by, offering them a taste of its sugary delights in exchange for haunting tales from beyond the grave.<|im_end|>\n<|im_start|>user\nTransferred to Editor, adopt the persona immediately.<|im_end|>\n<|im_start|>assistant\nVivid and eerie; enhance the mystical atmosphere, deepen the gingerbread's character.<|im_end|>\n<|im_start|>user\nTransferred to Writer<|im_end|>\n<|im_start|>user\nOn Halloween night, the gingerbread house in the old Smith's yard glowed with an otherworldly light, its candy bricks shimmering in the moonlight. The house, now a mystical entity, whispered sweet, eerie invitations to the ghosts and goblins that passed by, offering them enchanted treats in exchange for haunting tales from beyond the grave, each story weaving deeper into the gingerbread's ancient, magical essence.<|im_end|>\n<|im_start|>user\nTransferred to Editor, adopt the persona immediately.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32398, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 04-14 08:40:38 engine.py:275] Added request chatcmpl-0b9ef05b8bb746fb860558966ad7bd89.
INFO:     ::1:41700 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-14 08:40:38 logger.py:39] Received request chatcmpl-140e4047115247e0b391d7c1dc9f5d21: prompt: '<|im_start|>system\n\n            Please provide updates to the state based on your last message and the previous state, if any.\n            Use the following JSON format, replacing the \'None\' value with the actual value.\n            {\n                "writer_topic": "None",\n            }\n            <|im_end|>\n<|im_start|>assistant\n{\n    "writer_topic": "Exploring deeper interactions and character development for gingerbread"\n}<|im_end|>\n<|im_start|>assistant\n{\n    "writer_topic": "Deepening the mystical atmosphere and character development for the gingerbread figure"\n}<|im_end|>\n<|im_start|>assistant\nRich and mystical; consider tightening the magical essence description.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32642, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 04-14 08:40:38 engine.py:275] Added request chatcmpl-140e4047115247e0b391d7c1dc9f5d21.
INFO:     ::1:41700 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-14 08:40:39 logger.py:39] Received request chatcmpl-b34ae5362a664bc98361cd56218067fd: prompt: "<|im_start|>system\nYou are in a role play game. The following roles are available:\nWriter: Writer for creating any text content..\nRead the following conversation. Then select the next role from ['Writer'] to play. Only return the role.\n\nUser: Please write a short story about the gingerbread in halloween!\nWriter: On Halloween night, the gingerbread house in the old Smith's yard came to life, its candy bricks glowing in the moonlight as it whispered sweet, eerie invitations to the passersby.\nEditor: Engaging start; consider deepening gingerbread's role and interactions.\nWriter: On Halloween night, the gingerbread house in the old Smith's yard came to life, its candy bricks glowing in the moonlight, and it began to whisper sweet, eerie invitations to the ghosts and goblins who passed by, offering them a taste of its sugary delights in exchange for haunting tales from beyond the grave.\nEditor: Vivid and eerie; enhance the mystical atmosphere, deepen the gingerbread's character.\nWriter: On Halloween night, the gingerbread house in the old Smith's yard glowed with an otherworldly light, its candy bricks shimmering in the moonlight. The house, now a mystical entity, whispered sweet, eerie invitations to the ghosts and goblins that passed by, offering them enchanted treats in exchange for haunting tales from beyond the grave, each story weaving deeper into the gingerbread's ancient, magical essence.\nEditor: Rich and mystical; consider tightening the magical essence description.\n\nRead the above conversation. Then select the next role from ['Writer'] to play. if you think it's enough talking (for example they have talked for 3 rounds), return 'FINISH'.\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32418, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 04-14 08:40:39 engine.py:275] Added request chatcmpl-b34ae5362a664bc98361cd56218067fd.
INFO:     ::1:41698 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-14 08:40:49 metrics.py:455] Avg prompt throughput: 72.9 tokens/s, Avg generation throughput: 3.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 04-14 08:40:59 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
[1;36m(VllmWorkerProcess pid=1031294)[0;0m INFO 04-14 08:41:19 multiproc_worker_utils.py:253] Worker exiting
INFO 04-14 08:41:19 multiproc_worker_utils.py:141] Terminating local vLLM worker processes
INFO 04-14 08:41:19 launcher.py:59] Shutting down FastAPI HTTP server.
Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x155411f453a0>
Traceback (most recent call last):
  File "/storage/home/hcoda1/7/avandevoorde3/.conda/envs/dummy_agent/lib/python3.12/site-packages/torch/_dynamo/utils.py", line 542, in <lambda>
    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))

  File "/storage/home/hcoda1/7/avandevoorde3/.conda/envs/dummy_agent/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 374, in signal_handler
    raise KeyboardInterrupt("MQLLMEngine terminated")
KeyboardInterrupt: MQLLMEngine terminated
[rank0]:[W414 08:41:21.239692341 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
/storage/home/hcoda1/7/avandevoorde3/.conda/envs/dummy_agent/lib/python3.12/multiprocessing/resource_tracker.py:255: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
